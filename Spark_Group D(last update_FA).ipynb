{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Group D Assigment\n",
    "### An analysis on the U.S. states with relations to the time of departure and arrival to the respective airports (given the 'Flights Dataset', discussed in class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @Authors\n",
    " - Sandra Alemayehu\n",
    " - Frederico Andrade\n",
    " - Fernando Llopis\n",
    " - Amritesh Palani\n",
    " - Carmen Roldan\n",
    " - Bibake Uppal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"INDEX\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to the our Dataset (\"flights_jan08.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to a 2010 report made by the US Federal Aviation Administration, the economic price of domestic flight delays entails a yearly cost of 32.9 billion dollars to passengers, airlines and other parts of the economy. More than half of that amount comes from passengers' pockets, as they do not only waste time waiting for their planes to leave, but also miss connecting flights, spend money on food and have to sleep on hotel rooms while they're stranded.\n",
    "\n",
    "The report, focusing on data from year 2007, estimated that air transportation delays put a 4 billion dollar dent in the country's gross domestic product that year. Full report can be found \n",
    "<a href=\"http://www.isr.umd.edu/NEXTOR/pubs/TDI_Report_Final_10_18_10_V3.pdf\">here</a>.\n",
    "\n",
    "But which are the causes for these delays?\n",
    "\n",
    "In order to answer this question, we are going to analyze the provided dataset, containing up to 1.936.758 different internal flights in the US for 2008 and their causes for delay, diversion and cancellation; if any.\n",
    "\n",
    "The data comes from the U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS)\n",
    "\n",
    "This dataset is composed by the following variables:\n",
    "\n",
    "1. **Year** 2008\n",
    "2. **Month** 1\n",
    "3. **DayofMonth** 1-31\n",
    "4. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n",
    "5. **DepTime** actual departure time (local, hhmm)\n",
    "6. **CRSDepTime** scheduled departure time (local, hhmm)\n",
    "7. **ArrTime** actual arrival time (local, hhmm)\n",
    "8. **CRSArrTime** scheduled arrival time (local, hhmm)\n",
    "9. **UniqueCarrie**r unique carrier code\n",
    "10. **FlightNum** flight number\n",
    "11. **TailNum** plane tail number: aircraft registration, unique aircraft identifier\n",
    "12. **ActualElapsedTime** in minutes\n",
    "13. **CRSElapsedTime** in minutes\n",
    "14. **AirTime** in minutes\n",
    "15. **ArrDelay** arrival delay, in minutes: A flight is counted as \"on time\" if it operated less than 15 minutes later the scheduled time shown in the carriers' Computerized Reservations Systems (CRS).\n",
    "16. **DepDelay** departure delay, in minutes\n",
    "17. **Origin** origin IATA airport code\n",
    "18. **Dest** destination IATA airport code\n",
    "19. **Distance** in miles\n",
    "20. **TaxiIn** taxi in time, in minutes\n",
    "21. **TaxiOut** taxi out time in minutes\n",
    "22. **Cancelled** *was the flight cancelled\n",
    "23. **CancellationCode** reason for cancellation (A = carrier, B = weather, C = NAS, D = security)\n",
    "24. **Diverted** 1 = yes, 0 = no\n",
    "25. **CarrierDelay** in minutes: Carrier delay is within the control of the air carrier. Examples of occurrences that may determine carrier delay are: aircraft cleaning, aircraft damage, awaiting the arrival of connecting passengers or crew, baggage, bird strike, cargo loading, catering, computer, outage-carrier equipment, crew legality (pilot or attendant rest), damage by hazardous goods, engineering inspection, fueling, handling disabled passengers, late crew, lavatory servicing, maintenance, oversales, potable water servicing, removal of unruly passenger, slow boarding or seating, stowing carry-on baggage, weight and balance delays.\n",
    "26. **WeatherDelay** in minutes: Weather delay is caused by extreme or hazardous weather conditions that are forecasted or manifest themselves on point of departure, enroute, or on point of arrival.\n",
    "27. **NASDelay** in minutes: Delay that is within the control of the National Airspace System (NAS) may include: non-extreme weather conditions, airport operations, heavy traffic volume, air traffic control, etc.\n",
    "28. **SecurityDelay** in minutes: Security delay is caused by evacuation of a terminal or concourse, re-boarding of aircraft because of security breach, inoperative screening equipment and/or long lines in excess of 29 minutes at screening areas.\n",
    "29. **LateAircraftDelay** in minutes: Arrival delay at an airport due to the late arrival of the same aircraft at a previous airport. The ripple effect of an earlier delay at downstream airports is referred to as delay propagation\n",
    "30. **State** all 50 states of the United States.\n",
    "31. **Population** integer: total population of each state.\n",
    "32. **GDP per capita** integer: total gdp per capita for each state.\n",
    "33. **Unemployment rate** float: percentage of unemployment accross each state.\n",
    "34. **Poverty** integer: poverty rates for each state.\n",
    "35. **Agriculture** \n",
    "36. **Services**\n",
    "37. **Construction**\n",
    "38. **Financial Services**\n",
    "39. **Technology**\n",
    "40. **Industry**\n",
    "41. **Travelling and Tourism**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pyskark Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark-2.4.4-bin-hadoop2.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local[4]')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataFrame Setup & Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4c8b20583ce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mUSStates08\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The January Flights 2008 DataFrame has **%d rows**.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFlights08\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The United States 2008 DataFrame has **%d rows**.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mUSStates08\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Markdown' is not defined"
     ]
    }
   ],
   "source": [
    "Flights08 = spark.read\\\n",
    "                 .option(\"header\", \"true\")\\\n",
    "                 .option(\"inferSchema\", \"true\")\\\n",
    "                 .csv(\"flights_jan08.csv\")\n",
    "\n",
    "\n",
    "###For some reason this is not reading the data as it should: \n",
    "###please check below the difference between the first dataset and the second. \n",
    "###Sorry guys, but the dataset we have on the United States is being recognized as having only one column..###\n",
    "\n",
    "USStates08 = spark.read\\\n",
    "                .option(\"header\", \"false\")\\\n",
    "                .option(\"inferSchema\", \"true\")\\\n",
    "                .csv(\"US_States_dataset08.csv\")\n",
    "\n",
    "Flights08.printSchema()\n",
    "\n",
    "USStates08.printSchema()\n",
    "\n",
    "display(Markdown(\"The January Flights 2008 DataFrame has **%d rows**.\" % Flights08.count()))\n",
    "display(Markdown(\"The United States 2008 DataFrame has **%d rows**.\" % USStates08.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Joining the two Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Detected implicit cartesian product for INNER join between logical plans\\nRelation[Year#170,Month#171,DayofMonth#172,DayOfWeek#173,DepTime#174,CRSDepTime#175,ArrTime#176,CRSArrTime#177,UniqueCarrier#178,FlightNum#179,TailNum#180,ActualElapsedTime#181,CRSElapsedTime#182,AirTime#183,ArrDelay#184,DepDelay#185,Origin#186,Dest#187,Distance#188,TaxiIn#189,TaxiOut#190,Cancelled#191,CancellationCode#192,Diverted#193,... 5 more fields] csv\\nand\\nRelation[_c0#238] csv\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o99.showString.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans\nRelation[Year#170,Month#171,DayofMonth#172,DayOfWeek#173,DepTime#174,CRSDepTime#175,ArrTime#176,CRSArrTime#177,UniqueCarrier#178,FlightNum#179,TailNum#180,ActualElapsedTime#181,CRSElapsedTime#182,AirTime#183,ArrDelay#184,DepDelay#185,Origin#186,Dest#187,Distance#188,TaxiIn#189,TaxiOut#190,Cancelled#191,CancellationCode#192,Diverted#193,... 5 more fields] csv\nand\nRelation[_c0#238] csv\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$22.applyOrElse(Optimizer.scala:1295)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$22.applyOrElse(Optimizer.scala:1292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:259)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:259)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:264)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:264)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:264)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:264)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:264)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:264)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:248)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1292)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1274)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3365)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a6f840e2e06d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mFlightsStatesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mFlightsStatesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The joint flights DataFrame has **%d** rows.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFlightsStatesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for INNER join between logical plans\\nRelation[Year#170,Month#171,DayofMonth#172,DayOfWeek#173,DepTime#174,CRSDepTime#175,ArrTime#176,CRSArrTime#177,UniqueCarrier#178,FlightNum#179,TailNum#180,ActualElapsedTime#181,CRSElapsedTime#182,AirTime#183,ArrDelay#184,DepDelay#185,Origin#186,Dest#187,Distance#188,TaxiIn#189,TaxiOut#190,Cancelled#191,CancellationCode#192,Diverted#193,... 5 more fields] csv\\nand\\nRelation[_c0#238] csv\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'"
     ]
    }
   ],
   "source": [
    "### Here we have the same problem... with only one column recognized, I can't join both datasets###\n",
    "\n",
    "FlightsStatesDF = Flights08.join(USStates08)\n",
    "\n",
    "FlightsStatesDF.printSchema()\n",
    "FlightsStatesDF.show(5)\n",
    "\n",
    "display(Markdown(\"The joint flights DataFrame has **%d** rows.\" % FlightsStatesDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis (answering the proposed questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Relationship between the economic prosperity of a city and the proportion of flight arrivals during the weekdays and weekends:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's summarize our cities according to their economic prosperity.\n",
    "This operation is based on the available economic indicators (already addressed in the description of our dataset).\n",
    "Although we can rank our cities according to different and diverse indicators, we have chosen to represent our hierarchy according to the \"GDP Per Capita\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-963f4e093c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTotalGDPDF\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m    \u001b[0mflightsDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"State\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GDP per capita\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TotalGDP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcombinedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# optimization to make the processing faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count' is not defined"
     ]
    }
   ],
   "source": [
    "TotalGDPDF = \\\n",
    "   FlightsStatesDF.groupBy(\"State\")\\\n",
    "            .agg(count(\"GDP per capita\").alias(\"TotalGDP\"))\n",
    "\n",
    "combinedDF.cache() # optimization to make the processing faster\n",
    "\n",
    "display(Markdown(\"**States analysis on GDP Per Capita (in \\%):\"))\n",
    "TotalGDPDF.limit(50).show()\n",
    "TotalGDPDF\\\n",
    "   .groupBy(\"State\")\\\n",
    "   .limit(50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FlightsWeekDF = FlightsStatesDF.withColumn(\"DayOfWeek\",when((col(\"DayOfWeek\")>=1) & (col(\"DayOfWeek\")<=5),\"Weekdays\")\\\n",
    "                               .when((col(\"DayOfWeek\")>5) & (col(\"DayOfWeek\")<=7),\"Weekend\"))\n",
    "\n",
    "print(\"*Most 20: Flight By Destination: Week VS Weekend:*\")\n",
    "\n",
    "FlightsWeekDF.groupBy(\"Dest\").pivot(\"DayOfWeek\").count().orderBy(col(\"Weekdays\").desc()).show()\n",
    "\n",
    "print(\"*Least 20: Flight By Destination: Week VS Weekend:*\")\n",
    "\n",
    "CrimeWeekDF.groupBy(\"CRIME_TYPE\").pivot(\"DAY_OF_WEEK\").count().orderBy(col(\"Week\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Most and least frequent occurrences for FlightNum, TailNum, Origin and Dest columns:\")\n",
    "FlightNumDF = flightsDF.groupBy(\"FlightNum\").agg(count(lit(1)).alias(\"Total\"))\n",
    "TailNumDF   = flightsDF.groupBy(\"TailNum\").agg(count(lit(1)).alias(\"Total\"))\n",
    "OriginDF    = flightsDF.groupBy(\"Origin\").agg(count(lit(1)).alias(\"Total\"))\n",
    "DestDF      = flightsDF.groupBy(\"Dest\").agg(count(lit(1)).alias(\"Total\"))\n",
    "\n",
    "leastFreqFlightNum    = FlightNumDF.orderBy(col(\"Total\").asc()).first()\n",
    "mostFreqFlightNum     = FlightNumDF.orderBy(col(\"Total\").desc()).first()\n",
    "leastFreqTailNum      = TailNumDF.orderBy(col(\"Total\").asc()).first()\n",
    "mostFreqTailNum       = TailNumDF.orderBy(col(\"Total\").desc()).first()\n",
    "leastFreqOrigin       = OriginDF.orderBy(col(\"Total\").asc()).first()\n",
    "mostFreqOrigin        = OriginDF.orderBy(col(\"Total\").desc()).first()\n",
    "leastFreqDest         = DestDF.orderBy(col(\"Total\").asc()).first()\n",
    "mostFreqDest          = DestDF.orderBy(col(\"Total\").desc()).first()\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "| %s | %s | %s | %s |\n",
    "|----|----|----|----|\n",
    "| %s | %s | %s | %s |\n",
    "\"\"\" % (\"leastFreqFlightNum\", \"mostFreqFlightNum\", \"leastFreqTailNum\", \"mostFreqTailNum\", \\\n",
    "       \"%d (%d occurrences)\" % (leastFreqFlightNum[\"FlightNum\"], leastFreqFlightNum[\"Total\"]), \\\n",
    "       \"%d (%d occurrences)\" % (mostFreqFlightNum[\"FlightNum\"], mostFreqFlightNum[\"Total\"]), \\\n",
    "       \"%s (%d occurrences)\" % (leastFreqTailNum[\"TailNum\"], leastFreqTailNum[\"Total\"]), \\\n",
    "       \"%s (%d occurrences)\" % (mostFreqTailNum[\"TailNum\"], mostFreqTailNum[\"Total\"]))))\n",
    "display(Markdown(\"\"\"\n",
    "| %s | %s | %s | %s |\n",
    "|----|----|----|----|\n",
    "| %s | %s | %s | %s |\n",
    "\"\"\" % (\"leastFreqOrigin\", \"mostFreqOrigin\", \"leastFreqDest\", \"mostFreqDest\", \\\n",
    "       \"%s (%d occurrences)\" % (leastFreqOrigin[\"Origin\"], leastFreqOrigin[\"Total\"]), \\\n",
    "       \"%s (%d occurrences)\" % (mostFreqOrigin[\"Origin\"], mostFreqOrigin[\"Total\"]), \\\n",
    "       \"%s (%d occurrences)\" % (leastFreqDest[\"Dest\"], leastFreqDest[\"Total\"]), \\\n",
    "       \"%s (%d occurrences)\" % (mostFreqDest[\"Dest\"], mostFreqDest[\"Total\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Relationship between the business nature of a city and the proportion of flights that arrive early in the morning (e.g. with people in a business travel) with respect to the number of flights arriving during the rest of the day:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Proportion of flights arriving in the morning and the afternoon, when comparing to weekdays vs weekends in the aforementioned cities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Cities that receive most flights at weekends, and their connection to the vacation flow of customers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Do more developed cities suffer of smaller arrival delays on average?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Relationship between the arrival time and the arrival delay - categorization of the day into discrete parts, for the arrival time (is it the same for all categories of cities?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-33ee59fe8786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotalFlights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflightsDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdelayCategorizationDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflightsDF\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m    \u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ArrDelay\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"NA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m    .withColumn(\"DelaySeverity\", when(col(\"ArrDelay\")<=0,\"1.nodelay\")\\\n\u001b[1;32m      7\u001b[0m                                \u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ArrDelay\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ArrDelay\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2.acceptable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'col' is not defined"
     ]
    }
   ],
   "source": [
    "#CATEGORIZATION CODE#\n",
    "\n",
    "totalFlights = flightsDF.count()\n",
    "delayCategorizationDF = flightsDF\\\n",
    "   .where(col(\"ArrDelay\")!=\"NA\")\\\n",
    "   .withColumn(\"DelaySeverity\", when(col(\"ArrDelay\")<=0,\"1.nodelay\")\\\n",
    "                               .when((col(\"ArrDelay\")>0) & (col(\"ArrDelay\")<=15),\"2.acceptable\")\\\n",
    "                               .when((col(\"ArrDelay\")>15) & (col(\"ArrDelay\")<=30),\"3.annoying\")\\\n",
    "                               .when((col(\"ArrDelay\")>30) & (col(\"ArrDelay\")<=60),\"4.impactul\")\\\n",
    "                               .otherwise(\"5.unacceptable\"))\n",
    "\n",
    "#Arrival time VS Arrival Delay#\n",
    "\n",
    "severeDelaysDF = \\\n",
    "  delayCategorizationDF.where((col(\"Cancelled\")==0))\\\n",
    "                       .where((col(\"DelaySeverity\")!=\"1.nodelay\") & (col(\"DelaySeverity\")!=\"2.acceptable\"))\\\n",
    "                       .withColumn(\"IntArrDelay\", col(\"ArrDelay\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"IntCarrierDelay\", col(\"CarrierDelay\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"IntWeatherDelay\", col(\"WeatherDelay\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"IntNASDelay\", col(\"NASDelay\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"IntSecurityDelay\", col(\"SecurityDelay\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"IntLateAircraftDelay\", col(\"LateAircraftDelay\").cast(IntegerType()))\\\n",
    "                       .select(\"DelaySeverity\", \"IntArrDelay\",\"IntCarrierDelay\",\"IntWeatherDelay\",\\\n",
    "                               \"IntNASDelay\", \"IntSecurityDelay\", \"IntLateAircraftDelay\")\n",
    "severeDelaysDF.cache() # optimization to make the processing faster\n",
    "\n",
    "display(Markdown(\"**'Arrival' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntArrDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntArrDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntArrDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntArrDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()\n",
    "\n",
    "display(Markdown(\"**'Carrier' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntCarrierDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntCarrierDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntCarrierDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntCarrierDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()\n",
    "\n",
    "display(Markdown(\"**'Weather' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntWeatherDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntWeatherDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntWeatherDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntWeatherDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()\n",
    "\n",
    "display(Markdown(\"**'NAS' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntNASDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntNASDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntNASDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntNASDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()\n",
    "\n",
    "display(Markdown(\"**'Security' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntSecurityDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntSecurityDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntSecurityDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntSecurityDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()\n",
    "\n",
    "display(Markdown(\"**'LateAircraft' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntLateAircraftDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntLateAircraftDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntLateAircraftDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntLateAircraftDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. What about the departure time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DEPARTURE TIME##\n",
    "\n",
    "severeDelaysDF = \\\n",
    "  delayCategorizationDF.where((col(\"Cancelled\")==0))\\\n",
    "                       .where((col(\"DelaySeverity\")!=\"1.nodelay\") & (col(\"DelaySeverity\")!=\"2.acceptable\"))\\\n",
    "                       .withColumn(\"IntArrDelay\", col(\"ArrDelay\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"IntCarrierDelay\", col(\"CarrierDelay\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"IntWeatherDelay\", col(\"WeatherDelay\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"IntNASDelay\", col(\"NASDelay\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"IntSecurityDelay\", col(\"SecurityDelay\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"IntLateAircraftDelay\", col(\"LateAircraftDelay\").cast(IntegerType()))\\\n",
    "                       .select(\"DelaySeverity\", \"IntArrDelay\",\"IntCarrierDelay\",\"IntWeatherDelay\",\\\n",
    "                               \"IntNASDelay\", \"IntSecurityDelay\", \"IntLateAircraftDelay\")\n",
    "severeDelaysDF.cache() # optimization to make the processing faster\n",
    "\n",
    "display(Markdown(\"**'Arrival' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntArrDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntArrDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntArrDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntArrDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()\n",
    "\n",
    "display(Markdown(\"**'Carrier' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntCarrierDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntCarrierDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntCarrierDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntCarrierDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()\n",
    "\n",
    "display(Markdown(\"**'Weather' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntWeatherDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntWeatherDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntWeatherDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntWeatherDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()\n",
    "\n",
    "display(Markdown(\"**'NAS' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntNASDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntNASDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntNASDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntNASDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()\n",
    "\n",
    "display(Markdown(\"**'Security' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntSecurityDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntSecurityDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntSecurityDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntSecurityDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()\n",
    "\n",
    "display(Markdown(\"**'LateAircraft' severe delays basic stats** (in mins):\"))\n",
    "severeDelaysDF.groupBy(\"DelaySeverity\")\\\n",
    "              .agg(avg(\"IntLateAircraftDelay\").alias(\"AverageDelay\"),\\\n",
    "                   min(\"IntLateAircraftDelay\").alias(\"LowestDelay\"),\\\n",
    "                   max(\"IntLateAircraftDelay\").alias(\"HighestDelay\"),\\\n",
    "                   stddev(\"IntLateAircraftDelay\").alias(\"StdDevDelay\"))\\\n",
    "              .orderBy(\"DelaySeverity\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
